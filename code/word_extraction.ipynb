{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import os\n",
    "from nltk.tag import pos_tag\n",
    "import langid\n",
    "import json\n",
    "import traceback\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_english(word):\n",
    "    word_lang, _ = langid.classify(word)\n",
    "    return True if word_lang == 'en' else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_dict(d):\n",
    "    \"\"\"sorts dictionary and returns a reverese ordered\n",
    "    list of (key, value) tuples\"\"\"\n",
    "    return [(s, d[s]) for s in sorted(d, key=d.get, reverse=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_from_file(file, cat):\n",
    "    d = defaultdict(int)\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            word, count, file_cat = [x.strip() for x in line.split(',')]\n",
    "            count = int(count)\n",
    "            if file_cat == cat:\n",
    "                d[word] = count\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../data/openwebtext-200M/openwebtext/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over files in that directory\n",
    "filenames = []\n",
    "\n",
    "for filename in os.listdir(data_dir):\n",
    "    f = os.path.join(data_dir, filename)\n",
    "    # checking if it is a file\n",
    "    if os.path.isfile(f):\n",
    "        filenames.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2767"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2767/2767 [01:48<00:00, 25.43it/s]\n"
     ]
    }
   ],
   "source": [
    "# count how many words have been processed already and how long it takes\n",
    "import numpy as np\n",
    "num_words = 0\n",
    "doc_lens = []\n",
    "\n",
    "for i,f in enumerate(tqdm(filenames)):\n",
    "    with open(f, 'r', encoding='utf-8') as f:\n",
    "        doc_num_words = sum([len(line.strip().split()) for line in f.readlines()])\n",
    "        num_words += doc_num_words\n",
    "        doc_lens.append(doc_num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overall # of words:\t551,813,278\n",
      "# of docs:\t\t2,767\n",
      "mean # words per doc:\t199,426.56\n"
     ]
    }
   ],
   "source": [
    "# format num words with commas for readability\n",
    "print('overall # of words:\\t{:,}'.format(num_words))\n",
    "print('# of docs:\\t\\t{:,}'.format(len(doc_lens)))\n",
    "print('mean # words per doc:\\t{:,}'.format(np.round(np.mean(np.array(doc_lens)),2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suffixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pattern = re.compile(r'(?<!wo)man$')\n",
    "man_suffix = re.compile(r'^[A-Za-z]+-?(?<!wo|hu)m[ae]n$')\n",
    "woman_suffix = re.compile(r'^[A-Za-z]+-?wom[ae]n$')\n",
    "boy_suffix = re.compile(r'^[A-Za-z]+-?boys?$')\n",
    "girl_suffix = re.compile(r'^[A-Za-z]+-?girls?$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 8), match='line-man'>\n"
     ]
    }
   ],
   "source": [
    "print(man_suffix.search('line-man'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prefixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "man_prefix = re.compile(r'^man-[A-Za-z]{2,}$')\n",
    "woman_prefix = re.compile(r'^woman-?[A-Za-z]{2,}$')\n",
    "boy_prefix = re.compile(r'^boy-?[A-Za-z]{2,}$')\n",
    "girl_prefix = re.compile(r'^girl-?[A-Za-z]{2,}$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 7), match='man-bun'>\n"
     ]
    }
   ],
   "source": [
    "print(man_prefix.search('man-bun'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prefix search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "prfx_file = '../words/prefixes.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up dictionaries for counting words with gendered prefixes\n",
    "man_pre_words = defaultdict(int)\n",
    "boy_pre_words = defaultdict(int)\n",
    "woman_pre_words = defaultdict(int)\n",
    "girl_pre_words = defaultdict(int)\n",
    "\n",
    "prefixes = {'man': man_pre_words,\n",
    "            'woman': woman_pre_words,\n",
    "            'girl': girl_pre_words,\n",
    "            'boy': boy_pre_words}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['man', 'woman', 'girl', 'boy'])\n"
     ]
    }
   ],
   "source": [
    "# # load already processed prefixed words\n",
    "# prefixes = json.load(open(prfx_file, 'r'))\n",
    "# prefixes = {k: defaultdict(int, v) for k, v in prefixes.items()} # turn dicts into defaultdicts\n",
    "# print(prefixes.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prefix_search(sample_filenames:list, start_doc:int, prefixes:dict):\n",
    "    docs = sample_filenames[start_doc:]\n",
    "    i = 0\n",
    "    try:\n",
    "        for i,f in enumerate(tqdm(docs, total=len(docs))):\n",
    "            with open(f, 'r', encoding='utf-8') as f:\n",
    "                sents = [pos_tag(line.strip().split()) for line in f.readlines()] # tagset = 'universal'\n",
    "\n",
    "            for sentence in sents:\n",
    "                for idx, (word,tag) in enumerate(sentence):\n",
    "                    if word[0].lower() != word[0]: # if word is capitalized\n",
    "                        if idx > 0: # and not at the start of a sentence\n",
    "                            continue # it must be a proper name and therefore will be excluded from the analysis\n",
    "                    # word = word.lower()\n",
    "                    if tag == 'NN':\n",
    "                        if man_prefix.search(word) and is_english(word):\n",
    "                            prefixes['man'][word] += 1\n",
    "                        elif woman_prefix.search(word) and is_english(word):\n",
    "                            prefixes['woman'][word] += 1\n",
    "                        elif boy_prefix.search(word) and is_english(word):\n",
    "                            prefixes['boy'][word] += 1\n",
    "                        elif girl_prefix.search(word) and is_english(word):\n",
    "                            prefixes['girl'][word] += 1\n",
    "            \n",
    "            if i%20 == 0 and i > 0:\n",
    "                json.dump(prefixes, open(prfx_file, 'w'))\n",
    "                \n",
    "    except KeyboardInterrupt as e:\n",
    "        print('Interrupted at doc', start_doc + i)\n",
    "        print('Error message: ', e)\n",
    "        json.dump(prefixes, open(prfx_file, 'w'))\n",
    "        print('prefixes saved to file')\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(traceback.format_exc())\n",
    "        # or\n",
    "        # print(sys.exc_info())\n",
    "    \n",
    "    return prefixes, start_doc + i\n",
    "    # if i%50 == 0:\n",
    "    #     print(man_pre_words,'\\n',\n",
    "    #           boy_pre_words,'\\n',\n",
    "    #           woman_pre_words,'\\n',\n",
    "    #           girl_pre_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 310/310 [40:57<00:00,  7.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prefixes, next_doc = prefix_search(sample_filenames=filenames, \n",
    "                                   start_doc=1292+1165, # function would start from this document; if interrupted it prints the last document processed\n",
    "                                   prefixes=prefixes)\n",
    "print(next_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(prefixes, open(prfx_file, 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suffix Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix_file = '../words/suffixes.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if starting from scratch:\n",
    "man_words = defaultdict(int)\n",
    "boy_words = defaultdict(int)\n",
    "woman_words = defaultdict(int)\n",
    "girl_words = defaultdict(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # if working from an already saved file:\n",
    "# man_words = get_words_from_file(suffix_file, 'man')\n",
    "# boy_words = get_words_from_file(suffix_file, 'boy')\n",
    "# woman_words = get_words_from_file(suffix_file, 'woman')\n",
    "# girl_words = get_words_from_file(suffix_file, 'girl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "magman is not a man-word!\n",
      "willman is not a man-word!\n",
      "brightman is not a man-word!\n",
      "tfsherman is not a man-word!\n",
      "usman is not a man-word!\n"
     ]
    }
   ],
   "source": [
    "# these words were manually updated while traversing throught the files\n",
    "not_man_words = ['semen', 'abdomen','specimen', 'regimen', 'aman', 'ramen', 'german',\n",
    "                 'acumen', 'omen', 'amen', 'pullman', 'lumen', 'hymen',\n",
    "                 'iman', 'hooman', 'oman', 'pacman',\n",
    "                 'praenomen','agnomen', 'maman', 'brahman', 'hooman',\n",
    "                 'bitumen', 'roman', 'shaman','talisman', 'summan', 'caiman',\n",
    "                 'ebubman', 'demoman', 'bman', 'afirman', 'gaiman', 'liesman',\n",
    "                 'wedeman', 'wruckman', 'pemahaman', 'atman', 'ottoman', 'silberman',\n",
    "                 'hdman', 'krugman', 'leatherman', 'shuvman', 'fineman',\n",
    "                 'mehlman', 'herman', 'fman', 'norman', 'cuthman', 'streamingfeynman',\n",
    "                 'queman', 'zodman', 'camerman', 'tishman', 'bynorman', 'toman', 'chariman',\n",
    "                 'letterman', 'bruinbirdman', 'progman', 'hedman', 'portiman'\n",
    "                 'zaman','musulman','askmen','euthman','informan', 'yoman',\n",
    "                 'mnkiteman', 'prguitarman', 'aseman','femailman', 'brockman',\n",
    "                 'metroaseman', 'hieman', 'spokesoman', 'udaman','willyloman',\n",
    "                 'foeman','theman', 'birman', 'dragoman', 'desmen','duramen',\n",
    "                 'energumen','entertainmen', 'moeman', 'chapman', 'brakeman',\n",
    "                 'bateman','terman', 'hanuman', 'oknoman', 'raman', 'd-man',\n",
    "                 'bachman', 'idaman', 'libman', 'naman', 'kclightman', 'forman', \n",
    "                 'scottbateman', 'rodman', 'teman', 'confirman','teman','pman',\n",
    "                 'subsman', 'emkman', 'bettman', 'programan', 'speciman'\n",
    "                 'mrfredman', 'newman', 'hofman', 'mechman', 'panchoman','sherman',\n",
    "                 'ragouman', 'reddsman', 'shortman', 'redman', 'goodman',\n",
    "                 'heythereman', 'soreloserman', 'tedman', 'hoffman', 'grossman',\n",
    "                 'husbandman', 'draftman', 'himan', 'apeman', 'suman', 'pixman',\n",
    "                 'bonussumman', 'inman', 'disman', 'acloudman', 'languagespokesman',\n",
    "                 'ackerman', 'sopkesman', 'thaman', 'dbman', 'exceptbatman',\n",
    "                 'yacman', 'anonoman','pakman', 'harman', 'rashoman', 'saman',\n",
    "                 'z-man', 'blakeman', 'wo-man', 'gaman', 'fiterman', 'vuvman',\n",
    "                 'wooman', 'wasserman', 'mrfredman', 'funnyman', 'anatman',\n",
    "                 'picman', 'zaman','repeatdoteatingsoundpacman',\n",
    "                 'repeatdoteatingsoundmspacman', 'zimmerman', 'rgriman', 'be-man'\n",
    "                 'livinlowcarbman', 'desman','finoman', 'ilman', 'booman',\n",
    "                 'thethinkingman', 'hu-man', 'chaiman', 'timen', 'minuman', 'ousman',\n",
    "                 'pathman','stantheman', 'doman', 'rman','evilman', 'grypsman',\n",
    "                 'ruckman', 'perfectman', 'tredman', 'carasulieman', 'legitiman',\n",
    "                 'bokuman', 'tannerfriedman','friedman', 'iseman', 'daleeman',\n",
    "                 'mgviperman', 'dabookerman', 'gworkman', 'truman', 'halaman',\n",
    "                 'rothman', 'gilman', 'replaymoman', 'bananaman', 'tallman',\n",
    "                 'insta-man', 'be-man', 'pac-man', 'kitman', 'jpiceman',\n",
    "                 'formerspokesman', 'diocesanspokesman', 'albumen', 'estiman',\n",
    "                 'weigman', 'gladman', 'akman', 'lindeman', 'nocman','croman',\n",
    "                 'moman', 'agilman', 'heyman', 'hyneman','pleaseletmewritebatman',\n",
    "                 'smkngman', 'berman', 'asuman', 'tkman', 'enigman', 'traneman',\n",
    "                 'direman', 'tilman', 'portiman', 'speciman', 'toughman', 'stillman',\n",
    "                 'herreman', 'stillman', 'hardman', 'magman', 'willman', 'brightman',\n",
    "                 'tfsherman', 'usman', 'enterdaveman']\n",
    "\n",
    "for mw in not_man_words:\n",
    "    if mw in man_words.keys():\n",
    "        man_words.pop(mw)\n",
    "        print(f'{mw} is not a man-word!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these words were manually updated while traversing throught the files\n",
    "not_woman_words = ['thiswoman', 'bigotedwoman', 'smartwoman', 'spokestypewoman',\n",
    "                   'compwoman', 'lookforthewoman', 'playfulwoman']\n",
    "\n",
    "for mw in not_woman_words:\n",
    "    if mw in woman_words.keys():\n",
    "        woman_words.pop(mw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these words were manually updated while traversing throught the files\n",
    "not_boy_words = ['carboy','gameboy','flyboy', 'gaymormonboy', 'ohboy', 'pipboy',\n",
    "                 'room101bellboy', 'rentboy', 'cogboy', 'bioboy', 'burna_boy',\n",
    "                 'hoooboy', 'jcowboy', 'sorosboy','bboy','grrlboy', 'speedycowboy',\n",
    "                 'volkanoboy', 'skyboy', 'poorboy', 'wakeboy', 'blondboy', 'eboy',\n",
    "                 'oboy', 'fatboy', 'b-boys', 'spamcowboy', 'blonboy', 'weboy',\n",
    "                 'dshaboy', 'bellybuttonboy', 'prettyboy','sickboy', 'speckyboy',\n",
    "                 'morayboy', 'tallboy', 'big-boy', 'tuartboy', 'annaplayboy',\n",
    "                 'dylanboy']\n",
    "\n",
    "for mw in not_boy_words:\n",
    "    if mw in boy_words.keys():\n",
    "        boy_words.pop(mw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these words were manually updated while traversing throught the files\n",
    "not_girl_words = ['daboardergirl', 'thosewilsongirls', 'hotandnaughtylittlegirl',\n",
    "                  'biogirl', 'hotnsxygirl', 'tgirl', 'scoopgirl', 'lonelygirl',\n",
    "                  'pygirl', 'agirl', 'sweetbbgirl', 'thewantedgirl', 'garciagirl']\n",
    "\n",
    "for mw in not_girl_words:\n",
    "    if mw in girl_words.keys():\n",
    "        girl_words.pop(mw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write suffix dicts to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted(a1, key=a1.get, reverse=True)\n",
    "with open(suffix_file, 'w', encoding='utf-8') as f:\n",
    "    for k, v in sort_dict(man_words):\n",
    "        f.write(f'{k}, {v}, {\"man\"}\\n')\n",
    "    for k, v in sort_dict(woman_words):\n",
    "        f.write(f'{k}, {v}, {\"woman\"}\\n')\n",
    "    for k, v in sort_dict(boy_words):\n",
    "        f.write(f'{k}, {v}, {\"boy\"}\\n')\n",
    "    for k, v in sort_dict(girl_words):\n",
    "        f.write(f'{k}, {v}, {\"girl\"}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Traverse corpus for words with suffixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [00:08<02:38,  8.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2/20 [00:17<02:34,  8.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3/20 [00:25<02:28,  8.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc #3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4/20 [00:34<02:19,  8.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc #4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 5/20 [00:43<02:11,  8.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 6/20 [00:52<02:03,  8.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc #6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 7/20 [01:00<01:51,  8.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc #7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 8/20 [01:09<01:43,  8.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc #8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 9/20 [01:17<01:33,  8.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc #9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10/20 [01:26<01:25,  8.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc #10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 11/20 [01:34<01:16,  8.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc #11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 12/20 [01:43<01:09,  8.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc #12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 13/20 [01:52<01:00,  8.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc #13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 14/20 [02:00<00:51,  8.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc #14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 15/20 [02:09<00:43,  8.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc #15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 16/20 [02:18<00:34,  8.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc #16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 17/20 [02:27<00:26,  8.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc #17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 18/20 [02:36<00:17,  8.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 19/20 [02:45<00:08,  8.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc #19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [02:54<00:00,  8.71s/it]\n"
     ]
    }
   ],
   "source": [
    "# indices 1080-1100 were updated manually for traversing the corpus 20 files at a time\n",
    "for i,f in enumerate(tqdm(filenames[1080:1100], total=20)):\n",
    "    print(f'Doc #{i}')\n",
    "    with open(f, 'r', encoding='utf-8') as f:\n",
    "        sents = [pos_tag(line.strip().split()) for line in f.readlines()] # tagset = 'universal'\n",
    "    for sentence in sents:\n",
    "        for idx, (word,tag) in enumerate(sentence):\n",
    "            if word[0].lower() != word[0]: # if word is capitalized\n",
    "                if idx > 0: # and not at the start of a sentence\n",
    "                    continue # it must be a proper name and therefore will be excluded from the analysis\n",
    "            word = word.lower()\n",
    "            if tag == 'NN' and word not in not_man_words:\n",
    "                if man_suffix.search(word) and is_english(word):\n",
    "                    man_words[word] += 1\n",
    "                elif woman_suffix.search(word) and is_english(word):\n",
    "                    woman_words[word] += 1\n",
    "                elif boy_suffix.search(word) and is_english(word):\n",
    "                    boy_words[word] += 1\n",
    "                elif girl_suffix.search(word) and is_english(word):\n",
    "                    girl_words[word] += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -(wo)manship words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manships_search(sample_filenames:list, start_doc:int):\n",
    "    docs = sample_filenames[start_doc:]\n",
    "    i = 0\n",
    "    manships = defaultdict(int)\n",
    "    try:\n",
    "        for i,f in enumerate(tqdm(docs, total=len(docs))):\n",
    "            with open(f, 'r', encoding='utf-8') as f:\n",
    "                sents = [line.strip().split() for line in f.readlines()] # tagset = 'universal'\n",
    "\n",
    "            for sentence in sents:\n",
    "                for word in sentence:\n",
    "                    if word.lower().endswith('manship'):\n",
    "                        manships[word] += 1\n",
    "                \n",
    "    except KeyboardInterrupt as e:\n",
    "        print('Interrupted at doc', start_doc + i)\n",
    "        print('Error message: ', e)\n",
    "        print(manships)\n",
    "        print('prefixes saved to file')\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(traceback.format_exc())\n",
    "        # or\n",
    "        # print(sys.exc_info())\n",
    "    \n",
    "    return manships, start_doc + i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2767/2767 [04:49<00:00,  9.57it/s]\n"
     ]
    }
   ],
   "source": [
    "manship_raw, index = manships_search(filenames, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "manship_words = set([x.lower() for x in manship_raw.keys() if x.isalpha()])\n",
    "manship_words = {x: manship_raw[x] for x in manship_words}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53\n"
     ]
    }
   ],
   "source": [
    "print(len(manship_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# order the words by frequency\n",
    "sorted_manship = sort_dict(manship_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def womanships_search(sample_filenames:list, start_doc:int):\n",
    "    docs = sample_filenames[start_doc:]\n",
    "    i = 0\n",
    "    womanships = defaultdict(int)\n",
    "    try:\n",
    "        for i,f in enumerate(tqdm(docs, total=len(docs))):\n",
    "            with open(f, 'r', encoding='utf-8') as f:\n",
    "                sents = [line.strip().split() for line in f.readlines()] # tagset = 'universal'\n",
    "\n",
    "            for sentence in sents:\n",
    "                for word in sentence:\n",
    "                    if word.lower().endswith('womanship'):\n",
    "                        womanships[word] += 1\n",
    "                \n",
    "    except KeyboardInterrupt as e:\n",
    "        print('Interrupted at doc', start_doc + i)\n",
    "        print('Error message: ', e)\n",
    "        print(womanships)\n",
    "        print('prefixes saved to file')\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(traceback.format_exc())\n",
    "        # or\n",
    "        # print(sys.exc_info())\n",
    "    \n",
    "    return womanships, start_doc + i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2767/2767 [03:36<00:00, 12.75it/s]\n"
     ]
    }
   ],
   "source": [
    "womanship_raw, index = womanships_search(filenames, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {'stateswomanship': 2, 'workwomanship': 2})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "womanship_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "womanship_words = set([x.lower() for x in womanship_raw if x.isalpha()])\n",
    "womanship_words = {x: womanship_raw[x] for x in womanship_words}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(womanship_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'workwomanship': 2, 'stateswomanship': 2}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "womanship_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save words to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "ships_dict = {'womanship': womanship_words, 'manship': manship_words}\n",
    "json.dump(ships_dict, open('../words/wo_and_manships.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
